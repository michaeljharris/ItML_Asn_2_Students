{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Regression\n",
    "### Predict the TOTAL COMPENSATION for this year. \n",
    "\n",
    "The data file provided is a salary survey for tech workers in Europe. We want to predict the total amount of compensation they bring in each year, based off of the details of their work. \n",
    "\n",
    "Some notes that will be important:\n",
    "<ul>\n",
    "    <li>The total compensation will need to be constructed, there is a column for salary, \"Yearly brutto salary (without bonus and stocks) in EUR\", as well as a column for bonus compensation, \"Yearly bonus + stocks in EUR\". \n",
    "    <li>Some categorical variables will need some work, and there isn't generally an exact answer. The main concern is things with categories that have a bunch of values with a very small count. For example, if there is only 1 person in City X, then that value likely needs to be addressed. We don't want it encoded into a new column of one 1 and thousands of 0s. \n",
    "    <li>There is an article exploring some of the data here: https://www.asdcode.de/2021/01/it-salary-survey-december-2020.html\n",
    "    <li>Imputation and a bit of data manipulation will be required. \n",
    "    <li>Use any regression method you'd like. Some ones are closely related to what we've done, you may want to look at them, e.g. ExtraTreesRegressor. \n",
    "    <li>Initial accurracy, and potentially final accuracy, may not be great. When I made a plain model will little optimization the errors were large and the R2 was low. There is lots of room for optimization. \n",
    "    <li>Research challenge - try some work on the target, look into TransformedTargetRegressor and see if that helps. Recall in stats when we had skewed distributions... Maybe it helps, maybe it doesn't. \n",
    "    <li>EDA and data prep are up to you - you'll probably need to do a little exploring to figure out what cleanup is needed. When I did it, I did things kind of iteratively when I did it. For example, look at the value counts, figure out how to treat the different categories, clean something up, look at the results, potentially repeat if needed. After you figure out what needs to be done, you may be able to take some of those steps and incorporate them into a pipeline to be cleaner....\n",
    "    <li><b>CRITICAL - Please make sure your code runs with RUN ALL. It should load the data that you're given, do all the processing, and spit out results. Comment out or remove anything that you've cleaned up and don't need - e.g. if you scaled a value manually, then moved that into a pipeline, don't leave the original scaling code active when the file is run.</b>\n",
    "</ul>\n",
    "\n",
    "### Details and Deliverables\n",
    "\n",
    "You'll need to build code to produce the predictions. In particular, there's a few things that'll be marked:\n",
    "<ul>\n",
    "    <li>Please add a markdown cell at the bottom, and put in a few notes addressing the following:\n",
    "    <ul>\n",
    "        <li> Accuracy of your models with/without feature selection. Include both train/test for each. Please use R2 and RMSE. \n",
    "        <li> Feature Selection - Please identify what you did for feature selection. No need for a long explaination, something along the lines of \"I did X, and the result was that 4 features were removed\". Try at least 2 things. \n",
    "        <li> Hyperparameter Changes / Grid Search Improvements. What did you try, and why. Similar explaination to above, short. \n",
    "        <li> Overall this section should be roughly as long as this intro block - just outline what the results were, what you did to improve, and the results after. \n",
    "        <li> If you could use titles/bullet points I'd really appreciate it. \n",
    "    </ul>\n",
    "    <li>Grade Breakdown:\n",
    "    <ul>\n",
    "        <li> Code is readable, there are comments: 20%\n",
    "        <li> Explaination as defined above: 60% (20% each point)\n",
    "        <li> Accuracy: 20% As compared to everyone else. This will be generously graded, I won't be surprised if overall accuracy is low for most people. \n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"data/Euro_Salary.csv\")\n",
    "\n",
    "# Create target variable of total compensation\n",
    "df[\"Bonus\"] = pd.to_numeric(df[\"Yearly bonus + stocks in EUR\"], downcast=\"float\", errors=\"coerce\")\n",
    "df[\"Bonus\"].fillna(0, inplace=True)\n",
    "df[\"target\"] = df[\"Yearly brutto salary (without bonus and stocks) in EUR\"] + df[\"Bonus\"]\n",
    "df.drop(columns={\"Timestamp\",\"Yearly brutto salary (without bonus and stocks) in EUR\", \"Yearly bonus + stocks in EUR\", \"Bonus\"}, inplace=True)\n",
    "\n",
    "# Remove rows with more than 7 NaN values\n",
    "NaN_threshold = 7\n",
    "df = df[df.isnull().sum(axis=1) <= NaN_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning/Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the mode for \"Gender\" and \"Company size\" and using that to fill NaN values in those columns\n",
    "gender_mode = df['Gender'].mode()[0] \n",
    "df['Gender'].fillna(gender_mode, inplace=True)\n",
    "\n",
    "company_mode = df['Company size'].mode()[0] \n",
    "df['Company size'].fillna(company_mode, inplace=True)\n",
    "\n",
    "# Replacing low frequency values with \"Other\"\n",
    "def replace_low_freq(d, col, threshold=10, replacement='other'):\n",
    "    value_counts = d[col].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    tmp = d[col].replace(to_replace=to_remove, value=replacement)\n",
    "    return tmp\n",
    "\n",
    "# Using low frequency function on each categorical column, then filling NaN values with \"Other\"\n",
    "df[\"Seniority level\"] = replace_low_freq(df, \"Seniority level\", 5, \"Other\")\n",
    "df[\"Seniority level\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"City\"] = replace_low_freq(df, \"City\", 10, \"Other\")\n",
    "df[\"City\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Your main technology / programming language\"] = replace_low_freq(df, \"Your main technology / programming language\", 10, \"Other\")\n",
    "df[\"Your main technology / programming language\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Other technologies/programming languages you use often\"] = replace_low_freq(df, \"Other technologies/programming languages you use often\", 10, \"Other\")\n",
    "df[\"Other technologies/programming languages you use often\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Position \"] = replace_low_freq(df, \"Position \", 10, \"Other\")\n",
    "df[\"Position \"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Company type\"] = replace_low_freq(df, \"Company type\", 10, \"Other\")\n",
    "df[\"Company type\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Main language at work\"] = replace_low_freq(df, \"Main language at work\", 10, \"Other\")\n",
    "df[\"Main language at work\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Employment status\"] = replace_low_freq(df, \"Employment status\", 3, \"Other\")\n",
    "df[\"Employment status\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Contract duration\"] = replace_low_freq(df, \"Contract duration\", 3, \"Other\")\n",
    "df[\"Contract duration\"].fillna(\"Other\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an imputer to take the mean of the numerical columns for NaN values\n",
    "mean_columns = [\"Age\", \"Total years of experience\", \"Years of experience in Germany\", \"Number of vacation days\"]\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Replacing non-numerical symbols in data\n",
    "replacement = {\",\":\".\", \"<\":\" \"}\n",
    "df['Total years of experience'] = df['Total years of experience'].replace(replacement, regex=True)\n",
    "df['Years of experience in Germany'] = df['Years of experience in Germany'].replace(replacement, regex=True)\n",
    "\n",
    "# Converting column values to numerical to remove words\n",
    "df['Total years of experience'] = pd.to_numeric(df['Total years of experience'], errors='coerce')\n",
    "df['Years of experience in Germany'] = pd.to_numeric(df['Years of experience in Germany'], errors='coerce')\n",
    "df['Number of vacation days'] = pd.to_numeric(df['Number of vacation days'], errors='coerce')\n",
    "\n",
    "# Calculate and apply mean of column\n",
    "df[mean_columns] = imputer.fit_transform(df[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding for \"Seniority level\"\n",
    "seniority_order = {'Other': 0, 'Junior': 1, 'Middle': 2, 'Senior': 3, 'Lead': 4, 'Head': 5}\n",
    "df['Seniority level'] = df['Seniority level'].map(seniority_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1247, 67)\n",
      "(1247, 9)\n",
      "Index(['Age', 'Total years of experience', 'Years of experience in Germany',\n",
      "       'Seniority level', 'Number of vacation days',\n",
      "       'Position _Software Engineer',\n",
      "       'Your main technology / programming language_Other',\n",
      "       'Company size_101-1000', 'Company type_Product'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using Variance Threshold\n",
    "df_vf = pd.get_dummies(df, drop_first=True)\n",
    "y = df_vf[\"target\"]\n",
    "X = df_vf.drop(columns={\"target\"})\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_th = VarianceThreshold(.2)\n",
    "print(X.shape)\n",
    "post_vt = var_th.fit_transform(X)\n",
    "print(post_vt.shape)\n",
    "\n",
    "mask = var_th.get_support()\n",
    "new_features = X.columns[mask]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data frame after results of variance threshold feature selection\n",
    "df_fs1 = df.copy()\n",
    "df_fs1.drop(columns={\"Gender\", \"Other technologies/programming languages you use often\", \"Employment status\", \"Main language at work\", \"Contract duration\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data with one hot encoding\n",
    "#df1 = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "#y = df1[\"target\"]\n",
    "#X = df1.drop(columns={\"target\"})\n",
    "\n",
    "y = np.array(df[\"target\"]).reshape(-1,1)\n",
    "X = np.array(df.drop(columns={\"target\"}))\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(X,y,test_size=.3)\n",
    "\n",
    "numeric_features = [\"Age\", \"Total years of experience\", \"Years of experience in Germany\", \"Number of vacation days\"]  # List of column names with continuous numerical features\n",
    "categorical_features = [\"Gender\", \"City\", \"Position \", \"Seniority level\", \"Your main technology / programming language\", \"Other technologies/programming languages you use often\",\n",
    "                        \"Employment status\", \"Contract duration\", \"Main language at work\", \"Company size\", \"Company type\"]  # List of column names with categorical features\n",
    "\n",
    "# Create a transformer that applies MinMaxScaler to numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Create a transformer that applies OneHotEncoder to categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.17213609824355047\n",
      "Test RMSE: 79681473.0977757\n",
      "Test R2: -1747145.1946782991\n"
     ]
    }
   ],
   "source": [
    "#Train model with Linear Regression\n",
    "model = LinearRegression().fit(xTrain,yTrain)\n",
    "print(\"Training R2:\", model.score(xTrain,yTrain))\n",
    "\n",
    "#RMSE with test data\n",
    "model_preds = model.predict(xTest)\n",
    "print(\"Test RMSE:\", mean_squared_error(model_preds,yTest,squared=False))\n",
    "print(\"Test R2:\", model.score(xTest,yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.39573745030280283\n",
      "RMSE test: 422729069.66154927\n",
      "R2 test: -81547709.90535146\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding for all other categorical columns\n",
    "df_vf1 = pd.get_dummies(df_fs1, drop_first=True)\n",
    "y_vf = np.array(df_vf1[\"target\"]).reshape(-1,1)\n",
    "X_vf = np.array(df_vf1.drop(columns={\"target\"}))\n",
    "\n",
    "x_vfTrain,x_vfTest,y_vfTrain,y_vfTest = train_test_split(X_vf,y_vf,test_size=.3)\n",
    "\n",
    "#Train model with Linear Regression\n",
    "model_vf = LinearRegression().fit(x_vfTrain,y_vfTrain)\n",
    "print(\"Training R2:\", model_vf.score(x_vfTrain,y_vfTrain))\n",
    "\n",
    "#RMSE with test data\n",
    "model_vf_preds = model_vf.predict(x_vfTest)\n",
    "print(\"RMSE test:\", mean_squared_error(model_vf_preds,y_vfTest,squared=False))\n",
    "print(\"R2 test:\", model_vf.score(x_vfTest,y_vfTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'regression_tree' for estimator Pipeline(steps=[ColumnTransformer(transformers=[('num',\n                                                 Pipeline(steps=[('scaler',\n                                                                  MinMaxScaler())]),\n                                                 ['Age',\n                                                  'Total years of experience',\n                                                  'Years of experience in '\n                                                  'Germany',\n                                                  'Number of vacation days']),\n                                                ('cat',\n                                                 Pipeline(steps=[('onehot',\n                                                                  OneHotEncoder())]),\n                                                 ['Gender', 'City', 'Position ',\n                                                  'Seniority level',\n                                                  'Your main technology / '\n                                                  'programming language',\n                                                  'Other '\n                                                  'technologies/programming '\n                                                  'languages you use often',\n                                                  'Employment status',\n                                                  'Contract duration',\n                                                  'Main language at work',\n                                                  'Company size',\n                                                  'Company type'])]),\n                ('regression_tree', DecisionTreeRegressor())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 720, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 215, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 68, in _set_params\n    super().set_params(**params)\n  File \"c:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 229, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'regression_tree' for estimator Pipeline(steps=[ColumnTransformer(transformers=[('num',\n                                                 Pipeline(steps=[('scaler',\n                                                                  MinMaxScaler())]),\n                                                 ['Age',\n                                                  'Total years of experience',\n                                                  'Years of experience in '\n                                                  'Germany',\n                                                  'Number of vacation days']),\n                                                ('cat',\n                                                 Pipeline(steps=[('onehot',\n                                                                  OneHotEncoder())]),\n                                                 ['Gender', 'City', 'Position ',\n                                                  'Seniority level',\n                                                  'Your main technology / '\n                                                  'programming language',\n                                                  'Other '\n                                                  'technologies/programming '\n                                                  'languages you use often',\n                                                  'Employment status',\n                                                  'Contract duration',\n                                                  'Main language at work',\n                                                  'Company size',\n                                                  'Company type'])]),\n                ('regression_tree', DecisionTreeRegressor())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m tree_para \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression_tree__min_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m],\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression_tree__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m12\u001b[39m]}\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid\u001b[38;5;241m=\u001b[39mtree_para, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(xTrain, yTrain)\n\u001b[0;32m     12\u001b[0m model_tr \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_tr)\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\desla\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'regression_tree' for estimator Pipeline(steps=[ColumnTransformer(transformers=[('num',\n                                                 Pipeline(steps=[('scaler',\n                                                                  MinMaxScaler())]),\n                                                 ['Age',\n                                                  'Total years of experience',\n                                                  'Years of experience in '\n                                                  'Germany',\n                                                  'Number of vacation days']),\n                                                ('cat',\n                                                 Pipeline(steps=[('onehot',\n                                                                  OneHotEncoder())]),\n                                                 ['Gender', 'City', 'Position ',\n                                                  'Seniority level',\n                                                  'Your main technology / '\n                                                  'programming language',\n                                                  'Other '\n                                                  'technologies/programming '\n                                                  'languages you use often',\n                                                  'Employment status',\n                                                  'Contract duration',\n                                                  'Main language at work',\n                                                  'Company size',\n                                                  'Company type'])]),\n                ('regression_tree', DecisionTreeRegressor())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "#Regression Tree Model\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "regression_tree = DecisionTreeRegressor()\n",
    "pipe = Pipeline(steps=[(preprocessor), (\"regression_tree\", regression_tree)])\n",
    "\n",
    "tree_para = {'regression_tree__min_samples_split':[2,3,4,5,6,7],\n",
    "            'regression_tree__max_depth':[7,8,9,10,11,12]}\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid=tree_para, cv=5, n_jobs=-1)\n",
    "model.fit(xTrain, yTrain)\n",
    "model_tr = model.best_estimator_\n",
    "\n",
    "\n",
    "print(model_tr)\n",
    "\n",
    "print(\"Train Score:\", model.score(xTrain, yTrain))\n",
    "model_preds = model.predict(xTest)\n",
    "print(\"RMSE:\", mean_squared_error(model_preds,yTest,squared=False))\n",
    "print(\"R2:\", np.mean(cross_val_score(model_tr, xTrain, yTrain.ravel(), cv=5)))\n",
    "print(\"Test score:\", model.score(xTest, yTest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.14830265526050623\n",
      "RMSE: 71393328.110718\n",
      "R2: -2225523.622377915\n",
      "Test score: -890125.310682618\n"
     ]
    }
   ],
   "source": [
    "#Standard Gradient Descent Regression Model\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "regression_tree = SGDRegressor(max_iter=2000)\n",
    "pipe = Pipeline(steps=[(\"scalar\", scalar), (\"regression_tree\", regression_tree)])\n",
    "\n",
    "pipe.fit(xTrain, yTrain.ravel())\n",
    "\n",
    "\n",
    "\n",
    "#print(model_sgd)\n",
    "\n",
    "print(\"Train Score:\", pipe.score(xTrain, yTrain))\n",
    "model_preds = pipe.predict(xTest)\n",
    "print(\"RMSE:\", mean_squared_error(model_preds,yTest,squared=False))\n",
    "print(\"R2:\", np.mean(cross_val_score(pipe, xTrain, yTrain.ravel(), cv=5)))\n",
    "print(\"Test score:\", pipe.score(xTest, yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df_fs1, drop_first=True)\n",
    "\n",
    "#y = df1[\"target\"]\n",
    "#X = df1.drop(columns={\"target\"})\n",
    "\n",
    "y = np.array(df2[\"target\"]).reshape(-1,1)\n",
    "X = np.array(df2.drop(columns={\"target\"}))\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(X,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers and Explanations\n",
    "(Expand/modify as needed)\n",
    "\n",
    "### Results\n",
    "\n",
    "### Feature Selection Activities\n",
    "\n",
    "### Hyperparameter Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
