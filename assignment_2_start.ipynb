{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"data/Euro_Salary.csv\")\n",
    "\n",
    "# Create target variable of total compensation\n",
    "df[\"Bonus\"] = pd.to_numeric(df[\"Yearly bonus + stocks in EUR\"], downcast=\"float\", errors=\"coerce\")\n",
    "df[\"Bonus\"].fillna(0, inplace=True)\n",
    "df[\"target\"] = df[\"Yearly brutto salary (without bonus and stocks) in EUR\"] + df[\"Bonus\"]\n",
    "df.drop(columns={\"Timestamp\",\"Yearly brutto salary (without bonus and stocks) in EUR\", \"Yearly bonus + stocks in EUR\", \"Bonus\"}, inplace=True)\n",
    "\n",
    "# Remove rows with more than 7 NaN values\n",
    "NaN_threshold = 7\n",
    "df = df[df.isnull().sum(axis=1) <= NaN_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning/Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the mode for \"Gender\" and \"Company size\" and using that to fill NaN values in those columns\n",
    "gender_mode = df['Gender'].mode()[0] \n",
    "df['Gender'].fillna(gender_mode, inplace=True)\n",
    "\n",
    "company_mode = df['Company size'].mode()[0] \n",
    "df['Company size'].fillna(company_mode, inplace=True)\n",
    "\n",
    "# Replacing low frequency values with \"Other\"\n",
    "def replace_low_freq(d, col, threshold=10, replacement='other'):\n",
    "    value_counts = d[col].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    tmp = d[col].replace(to_replace=to_remove, value=replacement)\n",
    "    return tmp\n",
    "\n",
    "# Using low frequency function on each categorical column, then filling NaN values with \"Other\"\n",
    "df[\"Seniority level\"] = replace_low_freq(df, \"Seniority level\", 5, \"Other\")\n",
    "df[\"Seniority level\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"City\"] = replace_low_freq(df, \"City\", 10, \"Other\")\n",
    "df[\"City\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Your main technology / programming language\"] = replace_low_freq(df, \"Your main technology / programming language\", 10, \"Other\")\n",
    "df[\"Your main technology / programming language\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Other technologies/programming languages you use often\"] = replace_low_freq(df, \"Other technologies/programming languages you use often\", 10, \"Other\")\n",
    "df[\"Other technologies/programming languages you use often\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Position \"] = replace_low_freq(df, \"Position \", 10, \"Other\")\n",
    "df[\"Position \"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Company type\"] = replace_low_freq(df, \"Company type\", 10, \"Other\")\n",
    "df[\"Company type\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Main language at work\"] = replace_low_freq(df, \"Main language at work\", 10, \"Other\")\n",
    "df[\"Main language at work\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Employment status\"] = replace_low_freq(df, \"Employment status\", 3, \"Other\")\n",
    "df[\"Employment status\"].fillna(\"Other\", inplace=True)\n",
    "\n",
    "df[\"Contract duration\"] = replace_low_freq(df, \"Contract duration\", 3, \"Other\")\n",
    "df[\"Contract duration\"].fillna(\"Other\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an imputer to take the mean of the numerical columns for NaN values\n",
    "mean_columns = [\"Age\", \"Total years of experience\", \"Years of experience in Germany\", \"Number of vacation days\"]\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Replacing non-numerical symbols in data\n",
    "replacement = {\",\":\".\", \"<\":\" \"}\n",
    "df['Total years of experience'] = df['Total years of experience'].replace(replacement, regex=True)\n",
    "df['Years of experience in Germany'] = df['Years of experience in Germany'].replace(replacement, regex=True)\n",
    "\n",
    "# Converting column values to numerical to remove words\n",
    "df['Total years of experience'] = pd.to_numeric(df['Total years of experience'], errors='coerce')\n",
    "df['Years of experience in Germany'] = pd.to_numeric(df['Years of experience in Germany'], errors='coerce')\n",
    "df['Number of vacation days'] = pd.to_numeric(df['Number of vacation days'], errors='coerce')\n",
    "\n",
    "# Calculate and apply mean of column\n",
    "df[mean_columns] = imputer.fit_transform(df[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding for \"Seniority level\"\n",
    "seniority_order = {'Other': 0, 'Junior': 1, 'Middle': 2, 'Senior': 3, 'Lead': 4, 'Head': 5}\n",
    "df['Seniority level'] = df['Seniority level'].map(seniority_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1247, 9)\n",
      "Index(['Age', 'Total years of experience', 'Years of experience in Germany',\n",
      "       'Seniority level', 'Number of vacation days',\n",
      "       'Position _Software Engineer',\n",
      "       'Your main technology / programming language_Other',\n",
      "       'Company size_101-1000', 'Company type_Product'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using Variance Threshold\n",
    "df_vf = pd.get_dummies(df, drop_first=True)\n",
    "y_vf = df_vf[\"target\"]\n",
    "X_vf = df_vf.drop(columns={\"target\"})\n",
    "\n",
    "var_th = VarianceThreshold(.2)\n",
    "\n",
    "post_vt = var_th.fit_transform(X_vf)\n",
    "print(post_vt.shape)\n",
    "\n",
    "mask = var_th.get_support()\n",
    "new_features = X_vf.columns[mask]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data frame after results of variance threshold feature selection\n",
    "df_fs = df.copy()\n",
    "df_fs.drop(columns={\"Gender\", \"Other technologies/programming languages you use often\", \"Employment status\", \"Main language at work\", \"Contract duration\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for model testing\n",
    "df1 = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "y = np.array(df1[\"target\"]).reshape(-1,1)\n",
    "X = np.array(df1.drop(columns={\"target\"}))\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(X,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for model testing after feature selection\n",
    "df_new = pd.get_dummies(df_fs, drop_first=True)\n",
    "\n",
    "y_fs = np.array(df_new[\"target\"]).reshape(-1,1)\n",
    "X_fs = np.array(df_new.drop(columns={\"target\"}))\n",
    "\n",
    "xTrain_fs,xTest_fs,yTrain_fs,yTest_fs = train_test_split(X_fs,y_fs,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scalar', MinMaxScaler()),\n",
      "                ('regression_tree',\n",
      "                 DecisionTreeRegressor(criterion='absolute_error', max_depth=3,\n",
      "                                       min_samples_leaf=7,\n",
      "                                       min_samples_split=4))])\n",
      "Train Score with no feature selection: -0.0011589153013702447\n",
      "RMSE with no feature selection: 38155.93185325165\n",
      "R2 with no feature selection: 0.09263588866262706\n"
     ]
    }
   ],
   "source": [
    "#Regression Tree Model\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "regression_tree = DecisionTreeRegressor()\n",
    "pipe = Pipeline(steps=[('scalar', scalar), (\"regression_tree\", regression_tree)])\n",
    "\n",
    "tree_para = {'regression_tree__min_samples_leaf':[4,5,6,7,8],\n",
    "            'regression_tree__min_samples_split':[4,5,6,7,8],\n",
    "            'regression_tree__max_depth':[2,3,4,5,6,7],\n",
    "            'regression_tree__criterion':[\"friedman_mse\", \"poisson\", \"squared_error\", \"absolute_error\"]}\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid=tree_para, cv=5, n_jobs=-1)\n",
    "model.fit(xTrain, yTrain)\n",
    "model_tr = model.best_estimator_\n",
    "\n",
    "\n",
    "print(model_tr)\n",
    "\n",
    "print(\"Train Score with no feature selection:\", model.score(xTrain, yTrain))\n",
    "model_preds = model.predict(xTest)\n",
    "print(\"RMSE with no feature selection:\", mean_squared_error(yTest,model_preds,squared=False))\n",
    "print(\"R2 with no feature selection:\", np.mean(cross_val_score(model_tr, xTrain, yTrain.ravel(), cv=5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scalar', MinMaxScaler()),\n",
      "                ('regression_tree',\n",
      "                 DecisionTreeRegressor(criterion='absolute_error', max_depth=3,\n",
      "                                       min_samples_leaf=7,\n",
      "                                       min_samples_split=4))])\n",
      "Train Score after feature selection: 0.05729711774014179\n",
      "RMSE after feature selection: 284015832.4165602\n",
      "R2 after feature selection: 0.05522113574581004\n"
     ]
    }
   ],
   "source": [
    "#Regression Tree Model from data after feature selection\n",
    "\n",
    "scalar_fs = MinMaxScaler()\n",
    "regression_tree_fs = DecisionTreeRegressor()\n",
    "pipe_fs = Pipeline(steps=[('scalar', scalar_fs), (\"regression_tree\", regression_tree_fs)])\n",
    "\n",
    "tree_para_fs = {'regression_tree__min_samples_leaf':[4,5,6,7,8],\n",
    "            'regression_tree__min_samples_split':[4,5,6,7,8],\n",
    "            'regression_tree__max_depth':[2,3,4,5,6,7],\n",
    "            'regression_tree__criterion':[\"friedman_mse\", \"poisson\", \"squared_error\", \"absolute_error\"]}\n",
    "\n",
    "model_fs = GridSearchCV(pipe_fs, param_grid=tree_para_fs, cv=5, n_jobs=-1)\n",
    "model_fs.fit(xTrain_fs, yTrain_fs)\n",
    "model_tr_fs= model_fs.best_estimator_\n",
    "\n",
    "\n",
    "print(model_tr)\n",
    "\n",
    "print(\"Train Score after feature selection:\", model_fs.score(xTrain_fs, yTrain_fs))\n",
    "model_preds_fs = model_fs.predict(xTest_fs)\n",
    "print(\"RMSE after feature selection:\", mean_squared_error(yTest_fs,model_preds_fs,squared=False))\n",
    "print(\"R2 after feature selection:\", np.mean(cross_val_score(model_tr_fs, xTrain_fs, yTrain_fs.ravel(), cv=5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers and Explanations\n",
    "\n",
    "### Results\n",
    "- For my modelling I used a Regression Tree model with a MinMaxScalar, after using a OneHotEncoder to encode the categorical values through get_dummies. \n",
    "- The results of the model were pretty low for accuracy when using the intial data without any feature selection. For my intitial test R2 was 0.092 and RMSE was 38155.93. After using feature selection the results were an R2 of 0.055 and RMSE of 284015832.41. While the accuracy decreased slightly there was an increase in the errors, indicatng that the extra column may have been useful to the accuracy of the model, however the accuracy is still very low. Overall the issue of low accuracy and high errors may result from the way the data was imputed or cleaned initially, or perhaps a different and more complex model should be used.\n",
    "\n",
    "### Feature Selection Activities\n",
    "- For feature selection I used two methods: Inspection and Variance Threshold. \n",
    "- Through an initial inspection of the data and how it relates to the real situation, it makes sense that columns such as \"Gender\", \"Contract duration\", \"Main language at work\", and \"Other technologies/programming languages you use often\" would not really affect someones salary or bonuses in a job environment. The important columns for the model would definitely be relating to position, years of experience, and seniority level as these often affect someone's salary.\n",
    "- By using variance threshold, the columns that had low variance and therefore could be removed were \"Gender\", \"Other technologies/programming languages you use often\", \"Employment status\", \"Main language at work\", and \"Contract duration\".\n",
    "- Combining the results of both these feature selections lead to my new dataframe used for the second set of modelling.\n",
    "\n",
    "### Hyperparameter Changes\n",
    "- To reduce overfitting on the model I adjusted a few hyperparameters with the regression tree model, using a grid search to find the best combination. The parameters I used were min samples leaf and split, max depth, and criterion. The results found that having a lower depth and samples, as well as using absolute error would produce the best results for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
